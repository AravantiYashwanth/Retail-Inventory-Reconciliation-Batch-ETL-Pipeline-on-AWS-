
# ğŸ“¦ Retail Inventory Reconciliation (Batch ETL Pipeline on AWS)

![AWS](https://img.shields.io/badge/AWS-%23FF9900.svg?style=for-the-badge&logo=amazon-aws&logoColor=white)
![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![Apache Spark](https://img.shields.io/badge/Apache%20Spark-FDEE21?style=for-the-badge&logo=apachespark&logoColor=black)
![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-017CEE?style=for-the-badge&logo=apache-airflow&logoColor=white)
![Redshift](https://img.shields.io/badge/Amazon%20Redshift-8C4282?style=for-the-badge&logo=amazon-redshift&logoColor=white)
![Glue](https://img.shields.io/badge/AWS%20Glue-232F3E?style=for-the-badge&logo=amazon-aws&logoColor=white)
![QuickSight](https://img.shields.io/badge/Amazon%20QuickSight-232F3E?style=for-the-badge&logo=amazon-aws&logoColor=white)

---

## ğŸ“˜ Table of Contents
- [ğŸš€ Overview](#-overview)
- [ğŸ¯ Business Goal](#-business-goal)
- [ğŸ—ï¸ Solution Architecture](#ï¸-solution-architecture)
- [âš™ï¸ Technologies Used](#ï¸-technologies-used)
- [ğŸ§  Key Features & Logic](#-key-features--logic)
- [ğŸ“‚ Project Structure](#-project-structure)
- [ğŸ“Š Redshift Schema](#-redshift-schema)
- [ğŸ’¡ Learnings & Outcomes](#-learnings--outcomes)
- [ğŸ Future Enhancements](#-future-enhancements)
- [ğŸ‘¨â€ğŸ’» Author](#-author)

---

## ğŸš€ Overview

This project implements a **scalable batch ETL pipeline** on **AWS** to automate the process of **retail inventory reconciliation**.  
The pipeline ingests **daily Point-of-Sale (POS)** and **warehouse inventory data**, performs complex **transformations**, and loads curated data into **Amazon Redshift Serverless** for analytics.

The result: a complete end-to-end data engineering solution enabling **Amazon QuickSight** dashboards for:
- Real-time sales performance  
- Inventory tracking  
- Discrepancy detection between expected and actual stock  

---

## ğŸ¯ Business Goal

The primary objective is to empower business teams with **daily refreshed BI dashboards** in **Amazon QuickSight** to monitor:

- ğŸ§¾ **Sales Performance** â€“ Track daily sales and trends  
- ğŸ“Š **Product Trends** â€“ Identify top-selling products and categories  
- ğŸ¬ **Inventory Reconciliation** â€“ Detect mismatches between expected and actual stock  

---

## ğŸ—ï¸ Solution Architecture

The pipeline follows a **multi-layered Medallion-style data flow** built entirely on **AWS serverless components**.

```text
Amazon S3 (Raw Zone)
      â”‚
      â–¼
AWS Glue (ETL Jobs)
 â”œâ”€â”€ Job 1: Stage Sales Data
 â”œâ”€â”€ Job 2: Build Product Dimension
 â””â”€â”€ Job 3: Reconcile Inventory
      â”‚
      â–¼
Amazon Redshift (fact & dim tables)
      â”‚
      â–¼
Amazon QuickSight Dashboards
````

### ğŸ”„ Data Flow Summary

1. **Ingestion** â€“ Daily POS and inventory CSVs land in the **raw zone** of Amazon S3.
2. **Staging** â€“ **Glue Job 1** aggregates sales by product and stores in Parquet format.
3. **Transformation** â€“ **Glue Job 2 & 3** build product dimensions and calculate discrepancies:

   * `expected_closing_stock = opening_stock - total_quantity_sold`
   * `discrepancy = actual_closing_stock - expected_closing_stock`
4. **Alerting** â€“ If discrepancies exist, **Amazon SNS** sends automated notifications.
5. **Data Warehousing** â€“ Clean data is loaded into **Amazon Redshift Serverless**.
6. **Orchestration** â€“ Entire workflow managed via **Apache Airflow DAG**.
7. **Visualization** â€“ Dashboards in **Amazon QuickSight** provide insights to stakeholders.

---

## âš™ï¸ Technologies Used

| Category              | Technologies               | Purpose                                             |
| --------------------- | -------------------------- | --------------------------------------------------- |
| **Data Lake Storage** | Amazon S3                  | Central storage for raw, staged, and processed data |
| **ETL Processing**    | AWS Glue, PySpark          | Serverless transformation and reconciliation        |
| **Data Warehouse**    | Amazon Redshift Serverless | Analytics-ready schema for BI dashboards            |
| **Orchestration**     | Apache Airflow             | Schedule and manage dependent workflows             |
| **Alerting**          | Amazon SNS                 | Notify stakeholders of stock discrepancies          |
| **BI Visualization**  | Amazon QuickSight          | Interactive dashboards for business teams           |
| **IAM Security**      | AWS IAM                    | Manage secure service-to-service access             |

---

## ğŸ§  Key Features & Logic

### ğŸ§© Core Reconciliation Logic (PySpark)

```python
final_df = source_data.withColumn(
    "expected_closing_stock", (F.col("opening_stock") - F.col("total_quantity_sold"))
).withColumn(
    "discrepancy", (F.col("actual_closing_stock") - F.col("expected_closing_stock"))
)
```

### ğŸ—ï¸ Additional Features

* **Data Partitioning**: S3 data is partitioned by date (`date=YYYY-MM-DD`) for fast Athena/Redshift queries.
* **Idempotent Loads**: Airflow ensures re-runs overwrite data for the same processing date.
* **Scalability**: Fully serverless components scale automatically based on workload.
* **Dependency Management**: Airflow DAG enforces task execution order for consistent data freshness.
* **Error Handling**: SNS notifications on job failure or data mismatches.

---

## ğŸ“‚ Project Structure

```text
retail-inventory-reconciliation/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ retail_pipeline_dag.py          # Airflow DAG for orchestration
â”‚
â”œâ”€â”€ glue-scripts/
â”‚   â”œâ”€â”€ glue_job_1_stage_sales.py       # Aggregates raw POS sales
â”‚   â”œâ”€â”€ glue_job_create_dims.py         # Builds product dimension table
â”‚   â””â”€â”€ glue_job_2_reconcile.py         # Performs inventory reconciliation
â”‚
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ create_redshift_tables.sql      # DDL for Redshift schema
â”‚
â””â”€â”€ README.md
```

---

## ğŸ“Š Redshift Schema

### ğŸ§± Dimension: `dim_products`

```sql
CREATE TABLE IF NOT EXISTS dim_products (
    sku VARCHAR(50) NOT NULL,
    product_name VARCHAR(255),
    category VARCHAR(100),
    DISTSTYLE ALL,
    SORTKEY (sku)
);
```

### ğŸ“ˆ Fact: `fact_daily_sales`

```sql
CREATE TABLE IF NOT EXISTS fact_daily_sales (
    date_key DATE,
    sku VARCHAR(50),
    total_quantity_sold BIGINT,
    DISTKEY (sku),
    SORTKEY (date_key)
);
```

### ğŸ§® Fact: `fact_inventory_reconciliation`

```sql
CREATE TABLE IF NOT EXISTS fact_inventory_reconciliation (
    date_key DATE,
    sku VARCHAR(50),
    product_name VARCHAR(255),
    opening_stock INT,
    quantity_sold BIGINT,
    expected_closing_stock BIGINT,
    actual_closing_stock INT,
    discrepancy_amount BIGINT,
    DISTKEY(sku),
    SORTKEY(date_key)
);
```

---

## ğŸ’¡ Learnings & Outcomes

âœ… Built a **modular, serverless data pipeline** using modern AWS tools
âœ… Mastered **PySpark transformations** in AWS Glue
âœ… Implemented **Airflow orchestration** with clear task dependencies
âœ… Designed **data models (Star Schema)** optimized for analytics
âœ… Delivered **automated reconciliation & alerting** workflows
âœ… Enhanced understanding of **data warehouse optimization** in Redshift

---

## ğŸ Future Enhancements

* Integrate **AWS Lambda** for lightweight validation before ETL
* Add **Athena/Glue crawlers** for metadata catalog automation
* Expand alerting via **Slack or Microsoft Teams integrations**

---

## ğŸ‘¨â€ğŸ’» Author

**A. Yashwanth**
ğŸ“ *Electronics & Communication Engineer* | â˜ï¸ *Cloud & Data Enthusiast* | ğŸ’» *AWS + Python Developer*

[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue?style=flat\&logo=linkedin)](YOUR_LINKEDIN_URL)
[![GitHub](https://img.shields.io/badge/GitHub-Portfolio-black?style=flat\&logo=github)](YOUR_GITHUB_URL)


â­ *If you found this project helpful, consider giving it a star on GitHub!*


- `www.linkedin.com/in/yashwantharavanti` 



